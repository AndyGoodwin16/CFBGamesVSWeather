{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434b22b3-3d5b-406f-b793-f2b7c5c59c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies.\n",
    "from datetime import datetime\n",
    "from meteostat import Daily\n",
    "from meteostat import Point\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3bfd4c-be3f-450e-86f5-73614cab6043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#Imported cfb_attendance_editted.csv, which has manually cleaned data in \"Site\" column.\n",
    "df = pd.read_csv(\"cfb_attendance_editted.csv\", encoding = \"cp1252\")\n",
    "\n",
    "#Cleaning data (copy and pasted from data_clean.ipynb).\n",
    "df = df.astype({'Result': 'string'})\n",
    "df = df.astype({'Fill Rate': 'float64'})\n",
    "df = df.astype({'Conference': 'string'})\n",
    "\n",
    "df = df[df[\"Result\"].str.contains(\"Blue\") == False]\n",
    "df = df[df[\"Result\"].str.contains(\"White\") == False]\n",
    "df = df[df[\"Result\"].str.contains(\"NC\") == False]\n",
    "df = df[df[\"Result\"].str.contains(\"OT\") == False]\n",
    "df = df[df[\"Result\"].str.contains(\"vacated\") == False]\n",
    "df = df[df[\"Result\"].str.contains('\\[') == False]\n",
    "df = df[df[\"Result\"].str.contains('\\(') == False]\n",
    "df = df[df[\"Result\"].str.contains('\\â€¡') == False]\n",
    "df = df[df[\"Result\"].str.contains('A') == False]\n",
    "\n",
    "df['W/L'] = df['Result'].str.split(' ', expand=True)[0]\n",
    "df['Score'] = df['Result'].str.split(' ', expand=True)[1]\n",
    "df['Home Score'] = df['Score'].str.split('-', expand=True)[0]\n",
    "df['Away Score'] = df['Score'].str.split('-', expand=True)[1]\n",
    "\n",
    "df = df.astype({'Home Score': 'int64'})\n",
    "df = df.astype({'Away Score': 'int64'})\n",
    "\n",
    "tscore = df[\"Home Score\"] + df[\"Away Score\"]\n",
    "df[\"Total Score\"] = tscore\n",
    "\n",
    "#Splitting \"Site\" column to put City, State in new column.\n",
    "#Four different words (Stadium, Bowl, Field, Dome) appear where split needs to occur. Created four separate dataframes filtering for all but one of the words, \n",
    "#then splitting the data on that word. Did this for all four words, then concatenated the four dataframes.\n",
    "dfStadium = df[df[\"Site\"].str.contains(\"Stadium\")]\n",
    "dfStadium[\"City, State\"] = dfStadium[\"Site\"].str.split(\"Stadium\", expand=True)[1]\n",
    "dfBowl = df[df[\"Site\"].str.contains(\"Bowl\")]\n",
    "dfBowl[\"City, State\"] = dfBowl[\"Site\"].str.split(\"Bowl\", expand=True)[1]\n",
    "dfField = df[df[\"Site\"].str.contains(\"Field\")]\n",
    "dfField[\"City, State\"] = dfField[\"Site\"].str.split(\"Field\", expand=True)[1]\n",
    "dfDome = df[df[\"Site\"].str.contains(\"Dome\")]\n",
    "dfDome[\"City, State\"] = dfDome[\"Site\"].str.split(\"Dome\", expand=True)[1]\n",
    "\n",
    "merged_df = pd.concat([dfStadium, dfField, dfDome, dfBowl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd85019-6ab5-4507-96ae-aaa3731262ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of City, State data to iterate over.\n",
    "city_state_list = merged_df[\"City, State\"].tolist()\n",
    "#Create empty lists to store lat, lng data for each City, State.\n",
    "lat = []\n",
    "lng = []\n",
    "#Loop through all City, State data and retrieve lat, lng data through Google Maps API.\n",
    "for i in range(len(city_state_list)):\n",
    "    target_city_state = city_state_list[i]\n",
    "    target_url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={target_city_state}&key=AIzaSyC3eqFAAPDCogh9CSV4audVKKTFN3H9X2g\"\n",
    "    geo_data = requests.get(target_url).json()\n",
    "    lat.append(geo_data[\"results\"][0][\"geometry\"][\"location\"][\"lat\"])\n",
    "    lng.append(geo_data[\"results\"][0][\"geometry\"][\"location\"][\"lng\"])\n",
    "#Add lat, lng lists to dataframe.    \n",
    "merged_df[\"lat\"] = lat\n",
    "merged_df[\"lng\"] = lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81c817cb-1583-4e68-b536-444625a079d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists of month, day, year data to iterate over.\n",
    "month_list = merged_df[\"Month\"].tolist()\n",
    "day_list = merged_df[\"Day\"].tolist()\n",
    "year_list = merged_df[\"Year\"].tolist()\n",
    "#Create lists to store weather data for each City, State on the particular month, day, year.\n",
    "tavg = []\n",
    "tmin = []\n",
    "tmax = []\n",
    "prcp = []\n",
    "snow = []\n",
    "wdir = []\n",
    "wspd = []\n",
    "wpgt = []\n",
    "pres = []\n",
    "tsun = []\n",
    "#Loop through all City, State data and retrieve weather data from Meteostat API.\n",
    "for i in range(len(city_state_list)):\n",
    "    start = datetime(year_list[i], month_list[i], day_list[i])\n",
    "    end = datetime(year_list[i], month_list[i], day_list[i])\n",
    "    location = Point(lat[i], lng[i])\n",
    "    data = Daily(location, start, end)\n",
    "    data = data.fetch()\n",
    "    tavg.append(data[\"tavg\"].tolist())\n",
    "    tmin.append(data[\"tmin\"].tolist())\n",
    "    tmax.append(data[\"tmax\"].tolist())\n",
    "    prcp.append(data[\"prcp\"].tolist())\n",
    "    snow.append(data[\"snow\"].tolist())\n",
    "    wdir.append(data[\"wdir\"].tolist())\n",
    "    wspd.append(data[\"wspd\"].tolist())\n",
    "    wpgt.append(data[\"wpgt\"].tolist())\n",
    "    pres.append(data[\"pres\"].tolist())\n",
    "    tsun.append(data[\"tsun\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d5ce87-fb12-4ad1-ad70-32162b92990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not all data is available for every month, day, year, City, State.\n",
    "#Loop through weather data (currently a list of lists) and change all empty list values within the weather data lists to None valued lists.\n",
    "for i in range(len(tavg)):\n",
    "    if tavg[i] == []:\n",
    "        tavg[i] = [None]\n",
    "    if tmin[i] == []:\n",
    "        tmin[i] = [None]\n",
    "    if tmax[i] == []:\n",
    "        tmax[i] = [None]\n",
    "    if prcp[i] == []:\n",
    "        prcp[i] = [None]\n",
    "    if snow[i] == []:\n",
    "        snow[i] = [None]\n",
    "    if wdir[i] == []:\n",
    "        wdir[i] = [None]\n",
    "    if wspd[i] == []:\n",
    "        wspd[i] = [None]\n",
    "    if wpgt[i] == []:\n",
    "        wpgt[i] = [None]\n",
    "    if pres[i] == []:\n",
    "        pres[i] = [None]\n",
    "    if tsun[i] == []:\n",
    "        tsun[i] = [None]\n",
    "\n",
    "#Change list of lists to list of values using itertools.\n",
    "tavg = list(itertools.chain(*tavg))\n",
    "tmin = list(itertools.chain(*tmin))\n",
    "tmax = list(itertools.chain(*tmax))\n",
    "prcp = list(itertools.chain(*prcp))\n",
    "snow = list(itertools.chain(*snow))\n",
    "wdir = list(itertools.chain(*wdir))\n",
    "wspd = list(itertools.chain(*wspd))\n",
    "wpgt = list(itertools.chain(*wpgt))\n",
    "pres = list(itertools.chain(*pres))\n",
    "tsun = list(itertools.chain(*tsun))\n",
    "\n",
    "#Add weather data lists to dataframe.\n",
    "merged_df[\"tavg\"] = tavg\n",
    "merged_df[\"tmin\"] = tmin\n",
    "merged_df[\"tmax\"] = tmax\n",
    "merged_df[\"prcp\"] = prcp\n",
    "merged_df[\"snow\"] = snow\n",
    "merged_df[\"wdir\"] = wdir\n",
    "merged_df[\"wspd\"] = wspd\n",
    "merged_df[\"wpgt\"] = wpgt\n",
    "merged_df[\"pres\"] = pres\n",
    "merged_df[\"tsun\"] = tsun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a170474e-9917-4c3c-84ad-f1e401d1b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataframe to csv file.\n",
    "merged_df.to_csv(\"final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
